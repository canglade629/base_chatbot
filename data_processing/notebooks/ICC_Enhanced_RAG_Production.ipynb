{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ICC Enhanced RAG System - Production Deployment\n",
        "\n",
        "**Architecture:**\n",
        "- 🔍 **Enhanced Vector Search**: Dual-index retrieval with intelligent routing using `databricks-gte-large-en`\n",
        "- 🧠 **Advanced LLM**: `databricks-meta-llama-3-3-70b-instruct` for legal analysis\n",
        "- 🚀 **MLflow 3.0**: Production deployment and model management\n",
        "- ⚖️ **Legal Expertise**: Specialized for ICC defense team research\n",
        "\n",
        "**Data Sources:**\n",
        "- **Past Judgments Index**: `past_judgement` (ICTY/ICC case law)\n",
        "- **Geneva Documentation Index**: `geneva_documentation` (IHL framework)\n",
        "- **Vector Search Endpoint**: `jgmt` (with databricks-gte-large-en embedding model)\n",
        "\n",
        "**Key Features:**\n",
        "- Intelligent routing based on legal topics\n",
        "- Enhanced retrieval with relevance boosting\n",
        "- Comprehensive legal analysis generation\n",
        "- Production-ready MLflow 3.0 deployment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U -qqqq mlflow>=3.1.1 langchain databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks] uv databricks-feature-engineering==0.12.1\n",
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass, asdict\n",
        "import datetime\n",
        "import logging\n",
        "import re\n",
        "\n",
        "import mlflow\n",
        "from mlflow.models import infer_signature\n",
        "from mlflow.models.resources import (\n",
        "    DatabricksVectorSearchIndex,\n",
        "    DatabricksServingEndpoint\n",
        ")\n",
        "\n",
        "# Vector Search and LLM\n",
        "from databricks.vector_search.client import VectorSearchClient\n",
        "from databricks.sdk import WorkspaceClient\n",
        "from langchain_community.chat_models import ChatDatabricks\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "print(\"✅ Enhanced RAG dependencies loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced Configuration & Legal Topics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEPRECATED: This cell has been replaced by the updated configuration cell below\n",
        "# Enhanced Configuration - Updated Models\n",
        "VECTOR_SEARCH_ENDPOINT = \"jgmt\"  # Uses databricks-gte-large-en for semantic search\n",
        "PAST_JUDGMENTS_INDEX = \"past_judgement\"\n",
        "GENEVA_DOCUMENTATION_INDEX = \"geneva_documentation\"\n",
        "LLM_MODEL_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"  # Conversational LLM\n",
        "\n",
        "# Search parameters\n",
        "DEFAULT_TOP_K = 10\n",
        "MAX_CONTEXT_LENGTH = 4000\n",
        "SIMILARITY_THRESHOLD = 0.7\n",
        "MAX_TOKENS = 2048\n",
        "TEMPERATURE = 0.1\n",
        "\n",
        "# Legal topics for intelligent routing\n",
        "LEGAL_TOPICS = {\n",
        "    \"judgment_priority\": [\n",
        "        \"overall control\", \"state\", \"protected persons\", \"active participation\", \"direct participation\",\n",
        "        \"combatant status\", \"combatant privilege\", \"civilian status\", \"duty to protect\",\n",
        "        \"organisation of armed groups\", \"principle of distinction\", \"indiscriminate attack\",\n",
        "        \"civilian population\", \"military objectives\", \"military objects\", \"rule of proportionality\",\n",
        "        \"principle of proportionality\", \"collateral damage\", \"military necessity\",\n",
        "        \"military imperative\", \"security of civilians\", \"imperative military reasons\",\n",
        "        \"conduct of hostilities\", \"means of warfare\", \"methods of warfare\",\n",
        "        \"attacks against protected objects\", \"religious buildings\", \"displacement\",\n",
        "        \"deportation\", \"coercion\", \"cruel treatment\", \"torture\", \"outrages against dignity\",\n",
        "        \"murder\", \"self-defense\", \"causal link\", \"checkpoints\", \"roadblocks\",\n",
        "        \"icty\", \"trial chamber\", \"appeals chamber\", \"judgment\", \"applied\", \"practice\"\n",
        "    ],\n",
        "    \"geneva_priority\": [\n",
        "        \"geneva convention\", \"international humanitarian law\", \"ihl\", \"protected persons\",\n",
        "        \"wounded and sick\", \"prisoners of war\", \"civilians\", \"medical personnel\",\n",
        "        \"religious personnel\", \"cultural property\", \"distinctive emblems\", \"red cross\",\n",
        "        \"red crescent\", \"additional protocol\", \"grave breaches\", \"serious violations\",\n",
        "        \"customary international law\", \"treaty law\", \"convention\", \"protocol\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"✅ Enhanced configuration loaded with legal topics and updated models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Configuration\n",
        "VECTOR_SEARCH_ENDPOINT = \"jgmt\"\n",
        "PAST_JUDGMENTS_INDEX = \"past_judgement\"\n",
        "GENEVA_DOCUMENTATION_INDEX = \"geneva_documentation\"\n",
        "LLM_MODEL_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
        "\n",
        "# Search parameters\n",
        "DEFAULT_TOP_K = 10\n",
        "MAX_CONTEXT_LENGTH = 4000\n",
        "SIMILARITY_THRESHOLD = 0.7\n",
        "MAX_TOKENS = 2048\n",
        "TEMPERATURE = 0.1\n",
        "\n",
        "# Legal topics for intelligent routing\n",
        "LEGAL_TOPICS = {\n",
        "    \"judgment_priority\": [\n",
        "        \"overall control\", \"state\", \"protected persons\", \"active participation\", \"direct participation\",\n",
        "        \"combatant status\", \"combatant privilege\", \"civilian status\", \"duty to protect\",\n",
        "        \"organisation of armed groups\", \"principle of distinction\", \"indiscriminate attack\",\n",
        "        \"civilian population\", \"military objectives\", \"military objects\", \"rule of proportionality\",\n",
        "        \"principle of proportionality\", \"collateral damage\", \"military necessity\",\n",
        "        \"military imperative\", \"security of civilians\", \"imperative military reasons\",\n",
        "        \"conduct of hostilities\", \"means of warfare\", \"methods of warfare\",\n",
        "        \"attacks against protected objects\", \"religious buildings\", \"displacement\",\n",
        "        \"deportation\", \"coercion\", \"cruel treatment\", \"torture\", \"outrages against dignity\",\n",
        "        \"murder\", \"self-defense\", \"causal link\", \"checkpoints\", \"roadblocks\",\n",
        "        \"icty\", \"trial chamber\", \"appeals chamber\", \"judgment\", \"applied\", \"practice\"\n",
        "    ],\n",
        "    \"geneva_priority\": [\n",
        "        \"geneva convention\", \"international humanitarian law\", \"ihl\", \"protected persons\",\n",
        "        \"wounded and sick\", \"prisoners of war\", \"civilians\", \"medical personnel\",\n",
        "        \"religious personnel\", \"cultural property\", \"distinctive emblems\", \"red cross\",\n",
        "        \"red crescent\", \"additional protocol\", \"grave breaches\", \"serious violations\",\n",
        "        \"customary international law\", \"treaty law\", \"convention\", \"protocol\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"✅ Enhanced configuration loaded with legal topics\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced Data Structures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SearchResult:\n",
        "    \"\"\"Enhanced search result with comprehensive metadata\"\"\"\n",
        "    content: str\n",
        "    summary: str\n",
        "    source: str\n",
        "    metadata: Dict[str, Any]\n",
        "    score: float\n",
        "    source_type: str  # 'judgment' or 'geneva'\n",
        "    page_number: Optional[int] = None\n",
        "    article: Optional[str] = None\n",
        "    section: Optional[str] = None\n",
        "    document_type: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class RetrievalContext:\n",
        "    \"\"\"Enhanced retrieval context with routing information\"\"\"\n",
        "    question: str\n",
        "    routing_decision: str\n",
        "    judgment_results: List[SearchResult]\n",
        "    geneva_results: List[SearchResult]\n",
        "    all_results: List[SearchResult]\n",
        "    total_sources: int\n",
        "    processing_time: float\n",
        "    \n",
        "@dataclass\n",
        "class LegalAnalysis:\n",
        "    \"\"\"Structured legal analysis result\"\"\"\n",
        "    question: str\n",
        "    analysis: str\n",
        "    sources_used: List[SearchResult]\n",
        "    key_findings: List[str]\n",
        "    citations: List[str]\n",
        "    confidence_score: float\n",
        "    processing_time: float\n",
        "\n",
        "print(\"✅ Enhanced data structures defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced RAG System Core\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EnhancedICCRAGSystem:\n",
        "    \"\"\"Enhanced ICC RAG system with intelligent routing and legal expertise.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Initialize clients\n",
        "        self.vsc = VectorSearchClient()\n",
        "        self.w = WorkspaceClient()\n",
        "        self.llm = ChatDatabricks(\n",
        "            target_uri=\"databricks\",\n",
        "            endpoint=LLM_MODEL_ENDPOINT,\n",
        "            temperature=TEMPERATURE,\n",
        "            max_tokens=MAX_TOKENS\n",
        "        )\n",
        "        \n",
        "        # Conversation memory\n",
        "        self.conversations = {}\n",
        "        \n",
        "        # Legal terminology for query enhancement\n",
        "        self.legal_expansions = {\n",
        "            \"war crimes\": [\"war crime\", \"violations of laws of war\", \"grave breaches\"],\n",
        "            \"crimes against humanity\": [\"crime against humanity\", \"systematic attack\", \"persecution\"],\n",
        "            \"persecution\": [\"persecute\", \"persecuted\", \"discriminatory acts\", \"discriminatory intent\"],\n",
        "            \"murder\": [\"kill\", \"killing\", \"unlawful killing\", \"wilful killing\"],\n",
        "            \"active participation\": [\"direct participation\", \"hostilities\", \"combatant status\"],\n",
        "            \"civilian status\": [\"protected person\", \"civilian population\", \"non-combatant\"],\n",
        "            \"combatant status\": [\"combatant privilege\", \"armed forces\", \"military objective\"]\n",
        "        }\n",
        "    \n",
        "    def determine_routing_priority(self, question: str) -> str:\n",
        "        \"\"\"Determine which index to prioritize based on question content.\"\"\"\n",
        "        question_lower = question.lower()\n",
        "        \n",
        "        # Count matches for each topic category\n",
        "        judgment_matches = sum(1 for topic in LEGAL_TOPICS[\"judgment_priority\"] \n",
        "                              if topic in question_lower)\n",
        "        geneva_matches = sum(1 for topic in LEGAL_TOPICS[\"geneva_priority\"] \n",
        "                            if topic in question_lower)\n",
        "        \n",
        "        # Determine routing based on matches and question patterns\n",
        "        if \"icty\" in question_lower or \"trial\" in question_lower or \"appeal\" in question_lower:\n",
        "            return \"judgment\"\n",
        "        elif \"geneva\" in question_lower or \"convention\" in question_lower:\n",
        "            return \"geneva\"\n",
        "        elif judgment_matches > geneva_matches and judgment_matches > 0:\n",
        "            return \"judgment\"\n",
        "        elif geneva_matches > judgment_matches and geneva_matches > 0:\n",
        "            return \"geneva\"\n",
        "        elif judgment_matches > 0 and geneva_matches > 0:\n",
        "            return \"both\"\n",
        "        else:\n",
        "            return \"both\"  # Default to both if no clear indicators\n",
        "    \n",
        "    def enhance_query(self, query: str) -> str:\n",
        "        \"\"\"Enhance query for better retrieval using legal terminology.\"\"\"\n",
        "        enhanced = query.lower()\n",
        "        \n",
        "        # Add legal term expansions\n",
        "        for term, expansions in self.legal_expansions.items():\n",
        "            if term in enhanced:\n",
        "                enhanced += f\" {' '.join(expansions[:2])}\"\n",
        "        \n",
        "        return enhanced\n",
        "\n",
        "print(\"✅ Enhanced ICC RAG System core defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add search methods to the EnhancedICCRAGSystem class\n",
        "def add_search_methods_to_rag_system():\n",
        "    \"\"\"Add search methods to the RAG system class.\"\"\"\n",
        "    \n",
        "    def search_past_judgments(self, query: str, top_k: int = DEFAULT_TOP_K) -> List[SearchResult]:\n",
        "        \"\"\"Search past judgments using vector search with enhanced metadata.\"\"\"\n",
        "        try:\n",
        "            results = self.vsc.get_index(VECTOR_SEARCH_ENDPOINT, PAST_JUDGMENTS_INDEX).similarity_search(\n",
        "                query_text=query,\n",
        "                columns=[\"text\", \"summary\", \"source_file\", \"document_type\", \"section\", \"pages\"],\n",
        "                num_results=top_k\n",
        "            )\n",
        "            \n",
        "            search_results = []\n",
        "            for result in results:\n",
        "                # Extract page number if available\n",
        "                pages = result.get(\"pages\", [])\n",
        "                page_number = pages[0] if pages and len(pages) > 0 else None\n",
        "                \n",
        "                search_results.append(SearchResult(\n",
        "                    content=result.get(\"text\", \"\"),\n",
        "                    summary=result.get(\"summary\", \"\"),\n",
        "                    source=result.get(\"source_file\", \"Unknown\"),\n",
        "                    metadata={\n",
        "                        \"document_type\": result.get(\"document_type\", \"\"),\n",
        "                        \"section\": result.get(\"section\", \"\"),\n",
        "                        \"score\": result.get(\"score\", 0.0)\n",
        "                    },\n",
        "                    score=result.get(\"score\", 0.0),\n",
        "                    source_type=\"judgment\",\n",
        "                    page_number=page_number,\n",
        "                    section=result.get(\"section\", \"\"),\n",
        "                    document_type=result.get(\"document_type\", \"\")\n",
        "                ))\n",
        "            \n",
        "            return search_results\n",
        "        except Exception as e:\n",
        "            print(f\"Error searching past judgments: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def search_geneva_documentation(self, query: str, top_k: int = DEFAULT_TOP_K) -> List[SearchResult]:\n",
        "        \"\"\"Search Geneva Convention documentation using vector search.\"\"\"\n",
        "        try:\n",
        "            results = self.vsc.get_index(VECTOR_SEARCH_ENDPOINT, GENEVA_DOCUMENTATION_INDEX).similarity_search(\n",
        "                query_text=query,\n",
        "                columns=[\"text\", \"summary\", \"doc_name\", \"article\", \"section\", \"article_type\", \"pages\"],\n",
        "                num_results=top_k\n",
        "            )\n",
        "            \n",
        "            search_results = []\n",
        "            for result in results:\n",
        "                # Extract page number if available\n",
        "                pages = result.get(\"pages\", [])\n",
        "                page_number = pages[0] if pages and len(pages) > 0 else None\n",
        "                \n",
        "                search_results.append(SearchResult(\n",
        "                    content=result.get(\"text\", \"\"),\n",
        "                    summary=result.get(\"summary\", \"\"),\n",
        "                    source=result.get(\"doc_name\", \"Unknown\"),\n",
        "                    metadata={\n",
        "                        \"article\": result.get(\"article\", \"\"),\n",
        "                        \"section\": result.get(\"section\", \"\"),\n",
        "                        \"article_type\": result.get(\"article_type\", \"\"),\n",
        "                        \"score\": result.get(\"score\", 0.0)\n",
        "                    },\n",
        "                    score=result.get(\"score\", 0.0),\n",
        "                    source_type=\"geneva\",\n",
        "                    page_number=page_number,\n",
        "                    article=result.get(\"article\", \"\"),\n",
        "                    section=result.get(\"section\", \"\")\n",
        "                ))\n",
        "            \n",
        "            return search_results\n",
        "        except Exception as e:\n",
        "            print(f\"Error searching Geneva documentation: {e}\")\n",
        "            return []\n",
        "    \n",
        "    # Add methods to the class\n",
        "    EnhancedICCRAGSystem.search_past_judgments = search_past_judgments\n",
        "    EnhancedICCRAGSystem.search_geneva_documentation = search_geneva_documentation\n",
        "    \n",
        "    print(\"✅ Search methods added to Enhanced ICC RAG System\")\n",
        "\n",
        "# Execute the function to add methods\n",
        "add_search_methods_to_rag_system()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add retrieval and analysis methods\n",
        "def add_retrieval_methods_to_rag_system():\n",
        "    \"\"\"Add retrieval and analysis methods to the RAG system class.\"\"\"\n",
        "    \n",
        "    def retrieve_context(self, question: str, top_k: int = 5) -> RetrievalContext:\n",
        "        \"\"\"Enhanced retrieval function with intelligent routing.\"\"\"\n",
        "        start_time = datetime.datetime.now()\n",
        "        \n",
        "        print(f\"🔍 Retrieving context for: '{question[:100]}...'\")\n",
        "        \n",
        "        # Determine routing priority\n",
        "        routing_decision = self.determine_routing_priority(question)\n",
        "        print(f\"📊 Routing decision: {routing_decision}\")\n",
        "        \n",
        "        # Enhance query\n",
        "        enhanced_query = self.enhance_query(question)\n",
        "        \n",
        "        judgment_results = []\n",
        "        geneva_results = []\n",
        "        \n",
        "        # Search based on routing decision\n",
        "        if routing_decision in [\"judgment\", \"both\"]:\n",
        "            judgment_results = self.search_past_judgments(enhanced_query, top_k)\n",
        "            print(f\"⚖️ Found {len(judgment_results)} judgment results\")\n",
        "        \n",
        "        if routing_decision in [\"geneva\", \"both\"]:\n",
        "            geneva_results = self.search_geneva_documentation(enhanced_query, top_k)\n",
        "            print(f\"📜 Found {len(geneva_results)} Geneva Convention results\")\n",
        "        \n",
        "        # Combine and sort all results by score\n",
        "        all_results = judgment_results + geneva_results\n",
        "        all_results.sort(key=lambda x: x.score, reverse=True)\n",
        "        \n",
        "        # Filter by similarity threshold\n",
        "        filtered_results = [r for r in all_results if r.score >= SIMILARITY_THRESHOLD]\n",
        "        \n",
        "        processing_time = (datetime.datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        return RetrievalContext(\n",
        "            question=question,\n",
        "            routing_decision=routing_decision,\n",
        "            judgment_results=judgment_results,\n",
        "            geneva_results=geneva_results,\n",
        "            all_results=filtered_results,\n",
        "            total_sources=len(filtered_results),\n",
        "            processing_time=processing_time\n",
        "        )\n",
        "    \n",
        "    def generate_legal_analysis(self, question: str, context: RetrievalContext, \n",
        "                               conversation_id: str = None) -> LegalAnalysis:\n",
        "        \"\"\"Generate comprehensive legal analysis using LLM.\"\"\"\n",
        "        start_time = datetime.datetime.now()\n",
        "        \n",
        "        # Get or create conversation memory\n",
        "        if conversation_id:\n",
        "            if conversation_id not in self.conversations:\n",
        "                self.conversations[conversation_id] = ConversationBufferWindowMemory(\n",
        "                    k=5, return_messages=True\n",
        "                )\n",
        "            memory = self.conversations[conversation_id]\n",
        "            history = memory.chat_memory.messages\n",
        "        else:\n",
        "            history = []\n",
        "        \n",
        "        # Format contexts for LLM\n",
        "        context_text = self._format_contexts_for_analysis(context)\n",
        "        \n",
        "        # Create specialized legal system prompt\n",
        "        system_prompt = \"\"\"You are an expert legal analyst specializing in International Criminal Court (ICC) and International Criminal Tribunal for the former Yugoslavia (ICTY) proceedings. You are conducting comprehensive legal research for the ICC defense team.\n",
        "\n",
        "Your expertise includes:\n",
        "- International criminal law (war crimes, crimes against humanity, genocide)\n",
        "- ICTY and ICC procedures and legal standards\n",
        "- International humanitarian law (IHL) and Geneva Conventions\n",
        "- Legal reasoning and evidence evaluation\n",
        "- Combatant status, civilian status, and participation in hostilities\n",
        "\n",
        "Guidelines for your analysis:\n",
        "1. Base responses strictly on the provided legal context from judgments and conventions\n",
        "2. Use proper legal terminology and cite specific sections, articles, and page numbers\n",
        "3. Maintain judicial objectivity and professional legal analysis\n",
        "4. Provide comprehensive analysis with clear legal reasoning\n",
        "5. Extract and cite specific paragraphs when requested\n",
        "6. Identify key legal principles and their application\n",
        "7. Clearly distinguish between different legal sources (ICTY judgments vs Geneva Conventions)\n",
        "8. When analyzing subjective vs objective assessments, clearly identify the approach used\n",
        "\n",
        "Structure your response with:\n",
        "- Clear legal analysis based on the provided context\n",
        "- Specific citations with page numbers and sections\n",
        "- Key findings and legal principles\n",
        "- Relevant case law and precedents\n",
        "- Professional legal reasoning\n",
        "\n",
        "Always provide the full paragraphs when specifically requested and ensure all citations are accurate.\"\"\"\n",
        "        \n",
        "        # Create the prompt\n",
        "        chat_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            MessagesPlaceholder(variable_name=\"history\"),\n",
        "            (\"human\", \"\"\"\n",
        "Legal Research Context:\n",
        "{context}\n",
        "\n",
        "Legal Research Question: {query}\n",
        "\n",
        "Please provide a comprehensive legal analysis based on the provided context. If the question requests specific paragraphs, provide them in full with proper citations. Analyze the legal principles, identify key findings, and provide professional legal reasoning.\n",
        "\n",
        "For questions about subjective vs objective assessment, clearly identify which approach the chamber used and explain the legal reasoning behind it.\n",
        "\"\"\")\n",
        "        ])\n",
        "        \n",
        "        try:\n",
        "            # Format messages\n",
        "            messages = chat_template.format_messages(\n",
        "                context=context_text,\n",
        "                query=question,\n",
        "                history=history\n",
        "            )\n",
        "            \n",
        "            # Generate response\n",
        "            response = self.llm(messages)\n",
        "            \n",
        "            # Update memory if conversation_id provided\n",
        "            if conversation_id:\n",
        "                memory.chat_memory.add_user_message(question)\n",
        "                memory.chat_memory.add_ai_message(response.content)\n",
        "            \n",
        "            # Extract key findings and citations\n",
        "            key_findings = self._extract_key_findings(response.content)\n",
        "            citations = self._extract_citations(response.content)\n",
        "            \n",
        "            # Calculate confidence score based on source quality\n",
        "            confidence_score = self._calculate_confidence_score(context.all_results)\n",
        "            \n",
        "            processing_time = (datetime.datetime.now() - start_time).total_seconds()\n",
        "            \n",
        "            return LegalAnalysis(\n",
        "                question=question,\n",
        "                analysis=response.content,\n",
        "                sources_used=context.all_results,\n",
        "                key_findings=key_findings,\n",
        "                citations=citations,\n",
        "                confidence_score=confidence_score,\n",
        "                processing_time=processing_time\n",
        "            )\n",
        "            \n",
        "        except Exception as e:\n",
        "            return LegalAnalysis(\n",
        "                question=question,\n",
        "                analysis=f\"Error generating legal analysis: {str(e)}\",\n",
        "                sources_used=context.all_results,\n",
        "                key_findings=[],\n",
        "                citations=[],\n",
        "                confidence_score=0.0,\n",
        "                processing_time=(datetime.datetime.now() - start_time).total_seconds()\n",
        "            )\n",
        "    \n",
        "    def _format_contexts_for_analysis(self, context: RetrievalContext) -> str:\n",
        "        \"\"\"Format contexts for LLM processing with legal structure.\"\"\"\n",
        "        if not context.all_results:\n",
        "            return \"No relevant legal context found.\"\n",
        "        \n",
        "        formatted = []\n",
        "        \n",
        "        # Add judgment results\n",
        "        if context.judgment_results:\n",
        "            formatted.append(\"=== ICTY/ICC JUDGMENTS ===\")\n",
        "            for i, result in enumerate(context.judgment_results, 1):\n",
        "                formatted.append(f\"\"\"\n",
        "**Judgment Source {i}** - {result.document_type}\n",
        "- **Document**: {result.source}\n",
        "- **Section**: {result.section}\n",
        "- **Pages**: {result.page_number if result.page_number else 'N/A'}\n",
        "- **Relevance Score**: {result.score:.3f}\n",
        "\n",
        "**Content**:\n",
        "{result.content}\n",
        "\n",
        "**Summary**: {result.summary}\n",
        "---\"\"\")\n",
        "        \n",
        "        # Add Geneva Convention results\n",
        "        if context.geneva_results:\n",
        "            formatted.append(\"\\n=== GENEVA CONVENTIONS ===\")\n",
        "            for i, result in enumerate(context.geneva_results, 1):\n",
        "                formatted.append(f\"\"\"\n",
        "**Geneva Source {i}** - {result.metadata.get('article_type', 'N/A')}\n",
        "- **Document**: {result.source}\n",
        "- **Article**: {result.article}\n",
        "- **Section**: {result.section}\n",
        "- **Pages**: {result.page_number if result.page_number else 'N/A'}\n",
        "- **Relevance Score**: {result.score:.3f}\n",
        "\n",
        "**Content**:\n",
        "{result.content}\n",
        "\n",
        "**Summary**: {result.summary}\n",
        "---\"\"\")\n",
        "        \n",
        "        return \"\\n\".join(formatted)\n",
        "    \n",
        "    def _extract_key_findings(self, analysis: str) -> List[str]:\n",
        "        \"\"\"Extract key legal findings from the analysis.\"\"\"\n",
        "        # Simple extraction - in production, use more sophisticated NLP\n",
        "        findings = []\n",
        "        lines = analysis.split('\\n')\n",
        "        for line in lines:\n",
        "            if any(keyword in line.lower() for keyword in ['finding', 'principle', 'rule', 'established', 'determined']):\n",
        "                if len(line.strip()) > 20:  # Avoid very short lines\n",
        "                    findings.append(line.strip())\n",
        "        return findings[:5]  # Top 5 findings\n",
        "    \n",
        "    def _extract_citations(self, analysis: str) -> List[str]:\n",
        "        \"\"\"Extract legal citations from the analysis.\"\"\"\n",
        "        # Extract patterns like \"Article X\", \"Page Y\", \"Section Z\"\n",
        "        citation_patterns = [\n",
        "            r'Article\\s+\\d+[a-zA-Z]?',\n",
        "            r'Page\\s+\\d+',\n",
        "            r'Section\\s+[A-Z0-9]+',\n",
        "            r'ICC-[0-9]+-[0-9]+',\n",
        "            r'ICTY-[0-9]+-[0-9]+'\n",
        "        ]\n",
        "        \n",
        "        citations = []\n",
        "        for pattern in citation_patterns:\n",
        "            matches = re.findall(pattern, analysis, re.IGNORECASE)\n",
        "            citations.extend(matches)\n",
        "        \n",
        "        return list(set(citations))  # Remove duplicates\n",
        "    \n",
        "    def _calculate_confidence_score(self, sources: List[SearchResult]) -> float:\n",
        "        \"\"\"Calculate confidence score based on source quality.\"\"\"\n",
        "        if not sources:\n",
        "            return 0.0\n",
        "        \n",
        "        # Base score on average relevance and number of sources\n",
        "        avg_score = sum(s.score for s in sources) / len(sources)\n",
        "        source_bonus = min(len(sources) / 10, 0.3)  # Bonus for more sources, capped at 0.3\n",
        "        \n",
        "        return min(avg_score + source_bonus, 1.0)\n",
        "    \n",
        "    # Add methods to the class\n",
        "    EnhancedICCRAGSystem.retrieve_context = retrieve_context\n",
        "    EnhancedICCRAGSystem.generate_legal_analysis = generate_legal_analysis\n",
        "    EnhancedICCRAGSystem._format_contexts_for_analysis = _format_contexts_for_analysis\n",
        "    EnhancedICCRAGSystem._extract_key_findings = _extract_key_findings\n",
        "    EnhancedICCRAGSystem._extract_citations = _extract_citations\n",
        "    EnhancedICCRAGSystem._calculate_confidence_score = _calculate_confidence_score\n",
        "    \n",
        "    print(\"✅ Retrieval and analysis methods added to Enhanced ICC RAG System\")\n",
        "\n",
        "# Execute the function to add methods\n",
        "add_retrieval_methods_to_rag_system()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Legal Research Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the Enhanced RAG System with complex legal research questions\n",
        "def test_enhanced_rag_system():\n",
        "    \"\"\"Test the enhanced RAG system with the provided legal research questions.\"\"\"\n",
        "    \n",
        "    # Initialize the system\n",
        "    rag_system = EnhancedICCRAGSystem()\n",
        "    \n",
        "    # Complex legal research queries\n",
        "    test_questions = [\n",
        "        {\n",
        "            \"question\": \"Can you please go through all the ICTY trial judgments and appeal judgments and identify where the chamber discusses the status of an individual during the conflict. In particular, please identify all relevant paragraphs where the chamber refers to the active or direct participation of the individual or where the chamber discusses the civilian status or combatant status of an individual. Please provide the direct paragraph in full.\",\n",
        "            \"expected_routing\": \"judgment\",\n",
        "            \"key_topics\": [\"active participation\", \"direct participation\", \"civilian status\", \"combatant status\", \"ICTY\", \"trial judgments\", \"appeal judgments\"]\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Can you please go through all the ICTY trial judgments and appeal judgments and identify which factors the Trial or Appeals Chamber relied on in order to assess whether an individual is actively or directly participating in hostilities at a particular point? Please provide the full paragraph and citations\",\n",
        "            \"expected_routing\": \"judgment\", \n",
        "            \"key_topics\": [\"factors\", \"assessment\", \"actively participating\", \"directly participating\", \"hostilities\", \"Trial Chamber\", \"Appeals Chamber\", \"citations\"]\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Can you please search through all the ICTY trial judgments and appeal judgments and identify relevant paragraphs which would support the proposition that an individual who has previously joined enemy forces and is armed at the relevant point is considered to have lost their protected status at a particular point? Please determine whether the chamber undertakes a subjective or objective assessment?\",\n",
        "            \"expected_routing\": \"judgment\",\n",
        "            \"key_topics\": [\"enemy forces\", \"armed\", \"protected status\", \"subjective assessment\", \"objective assessment\", \"lost status\"]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    print(\"🧪 TESTING ENHANCED ICC RAG SYSTEM\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for i, query_info in enumerate(test_questions, 1):\n",
        "        print(f\"\\n{'#'*80}\")\n",
        "        print(f\"LEGAL RESEARCH QUESTION {i}\")\n",
        "        print(f\"{'#'*80}\")\n",
        "        print(f\"Question: {query_info['question'][:150]}...\")\n",
        "        print(f\"Expected routing: {query_info['expected_routing']}\")\n",
        "        print(f\"Key topics: {', '.join(query_info['key_topics'])}\")\n",
        "        \n",
        "        # Retrieve context\n",
        "        context = rag_system.retrieve_context(query_info[\"question\"], top_k=8)\n",
        "        \n",
        "        # Generate legal analysis\n",
        "        analysis = rag_system.generate_legal_analysis(\n",
        "            query_info[\"question\"], \n",
        "            context, \n",
        "            conversation_id=f\"test_session_{i}\"\n",
        "        )\n",
        "        \n",
        "        # Display results\n",
        "        print(f\"\\n📊 ROUTING ANALYSIS:\")\n",
        "        print(f\"Expected: {query_info['expected_routing']}\")\n",
        "        print(f\"Actual: {context.routing_decision}\")\n",
        "        print(f\"Sources found: {context.total_sources}\")\n",
        "        print(f\"Processing time: {context.processing_time:.2f}s\")\n",
        "        \n",
        "        print(f\"\\n⚖️ LEGAL ANALYSIS:\")\n",
        "        print(f\"Confidence score: {analysis.confidence_score:.3f}\")\n",
        "        print(f\"Key findings: {len(analysis.key_findings)}\")\n",
        "        print(f\"Citations: {len(analysis.citations)}\")\n",
        "        print(f\"Analysis length: {len(analysis.analysis)} characters\")\n",
        "        \n",
        "        print(f\"\\n📝 ANALYSIS PREVIEW:\")\n",
        "        print(analysis.analysis[:500] + \"...\" if len(analysis.analysis) > 500 else analysis.analysis)\n",
        "        \n",
        "        print(f\"\\n🔍 KEY FINDINGS:\")\n",
        "        for j, finding in enumerate(analysis.key_findings[:3], 1):\n",
        "            print(f\"{j}. {finding}\")\n",
        "        \n",
        "        print(f\"\\n📚 CITATIONS:\")\n",
        "        for j, citation in enumerate(analysis.citations[:5], 1):\n",
        "            print(f\"{j}. {citation}\")\n",
        "        \n",
        "        results.append({\n",
        "            \"question_id\": i,\n",
        "            \"question\": query_info[\"question\"],\n",
        "            \"routing_decision\": context.routing_decision,\n",
        "            \"sources_found\": context.total_sources,\n",
        "            \"confidence_score\": analysis.confidence_score,\n",
        "            \"analysis_length\": len(analysis.analysis),\n",
        "            \"key_findings_count\": len(analysis.key_findings),\n",
        "            \"citations_count\": len(analysis.citations),\n",
        "            \"processing_time\": context.processing_time + analysis.processing_time\n",
        "        })\n",
        "        \n",
        "        print(f\"\\n{'#'*80}\\n\")\n",
        "    \n",
        "    # Summary\n",
        "    print(\"📊 TEST SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    for result in results:\n",
        "        print(f\"Question {result['question_id']}: {result['routing_decision']} routing, \"\n",
        "              f\"{result['sources_found']} sources, {result['confidence_score']:.3f} confidence, \"\n",
        "              f\"{result['processing_time']:.2f}s\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run the test\n",
        "test_results = test_enhanced_rag_system()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLflow 3.0 Production Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EnhancedICCRAGModel(mlflow.pyfunc.PythonModel):\n",
        "    \"\"\"MLflow 3.0 production model wrapper for Enhanced ICC RAG System.\"\"\"\n",
        "    \n",
        "    def load_context(self, context):\n",
        "        \"\"\"Initialize the enhanced RAG system.\"\"\"\n",
        "        self.rag_system = EnhancedICCRAGSystem()\n",
        "    \n",
        "    def predict(self, context, model_input: pd.DataFrame) -> List[Dict]:\n",
        "        \"\"\"Handle predictions for serving endpoint.\"\"\"\n",
        "        try:\n",
        "            queries = model_input[\"query\"].tolist()\n",
        "            \n",
        "            # Extract optional parameters\n",
        "            num_results_list = model_input.get(\"num_results\", [8] * len(queries)).tolist()\n",
        "            conversation_ids = model_input.get(\"conversation_id\", [None] * len(queries)).tolist()\n",
        "            \n",
        "            results = []\n",
        "            for query, num_results, conv_id in zip(queries, num_results_list, conversation_ids):\n",
        "                try:\n",
        "                    # Retrieve context\n",
        "                    context = self.rag_system.retrieve_context(\n",
        "                        query=query,\n",
        "                        top_k=num_results if pd.notna(num_results) else 8\n",
        "                    )\n",
        "                    \n",
        "                    # Generate legal analysis\n",
        "                    analysis = self.rag_system.generate_legal_analysis(\n",
        "                        question=query,\n",
        "                        context=context,\n",
        "                        conversation_id=conv_id if pd.notna(conv_id) else None\n",
        "                    )\n",
        "                    \n",
        "                    # Format response\n",
        "                    result = {\n",
        "                        \"question\": query,\n",
        "                        \"analysis\": analysis.analysis,\n",
        "                        \"routing_decision\": context.routing_decision,\n",
        "                        \"sources_used\": len(analysis.sources_used),\n",
        "                        \"confidence_score\": analysis.confidence_score,\n",
        "                        \"key_findings\": analysis.key_findings,\n",
        "                        \"citations\": analysis.citations,\n",
        "                        \"processing_time_seconds\": context.processing_time + analysis.processing_time,\n",
        "                        \"conversation_id\": conv_id,\n",
        "                        \"sources\": [\n",
        "                            {\n",
        "                                \"source\": s.source,\n",
        "                                \"source_type\": s.source_type,\n",
        "                                \"section\": s.section,\n",
        "                                \"page_number\": s.page_number,\n",
        "                                \"article\": s.article,\n",
        "                                \"relevance_score\": round(s.score, 3)\n",
        "                            }\n",
        "                            for s in analysis.sources_used[:10]  # Top 10 sources\n",
        "                        ]\n",
        "                    }\n",
        "                    results.append(result)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    # Handle individual query errors\n",
        "                    error_result = {\n",
        "                        \"question\": query,\n",
        "                        \"analysis\": f\"Error processing query: {str(e)}\",\n",
        "                        \"routing_decision\": \"error\",\n",
        "                        \"sources_used\": 0,\n",
        "                        \"confidence_score\": 0.0,\n",
        "                        \"key_findings\": [],\n",
        "                        \"citations\": [],\n",
        "                        \"processing_time_seconds\": 0,\n",
        "                        \"conversation_id\": conv_id,\n",
        "                        \"sources\": []\n",
        "                    }\n",
        "                    results.append(error_result)\n",
        "            \n",
        "            return results\n",
        "            \n",
        "        except Exception as e:\n",
        "            return [{\"error\": f\"Model error: {str(e)}\"}] * len(model_input)\n",
        "\n",
        "print(\"✅ Enhanced ICC RAG Model for MLflow 3.0 defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register the Enhanced ICC RAG Model in MLflow 3.0\n",
        "with mlflow.start_run(run_name=\"Enhanced_ICC_RAG_Production\") as run:\n",
        "    \n",
        "    # Create model instance\n",
        "    production_model = EnhancedICCRAGModel()\n",
        "    \n",
        "    # Input example for serving endpoint\n",
        "    input_example = pd.DataFrame({\n",
        "        \"query\": [\n",
        "            \"Can you please go through all the ICTY trial judgments and identify where the chamber discusses the status of an individual during the conflict?\",\n",
        "            \"What factors did the Trial Chamber rely on to assess active participation in hostilities?\"\n",
        "        ],\n",
        "        \"num_results\": [10, 12],\n",
        "        \"conversation_id\": [\"legal_research_001\", \"legal_research_001\"]\n",
        "    })\n",
        "    \n",
        "    # Expected output format\n",
        "    output_example = [\n",
        "        {\n",
        "            \"question\": \"Sample legal question\",\n",
        "            \"analysis\": \"Comprehensive legal analysis based on retrieved context...\",\n",
        "            \"routing_decision\": \"judgment\",\n",
        "            \"sources_used\": 8,\n",
        "            \"confidence_score\": 0.85,\n",
        "            \"key_findings\": [\"Key legal finding 1\", \"Key legal finding 2\"],\n",
        "            \"citations\": [\"Article 8\", \"Page 123\", \"Section A\"],\n",
        "            \"processing_time_seconds\": 5.2,\n",
        "            \"conversation_id\": \"legal_research_001\",\n",
        "            \"sources\": [\n",
        "                {\n",
        "                    \"source\": \"ICTY_Judgment_001.pdf\",\n",
        "                    \"source_type\": \"judgment\",\n",
        "                    \"section\": \"FINDINGS_OF_FACT\",\n",
        "                    \"page_number\": 123,\n",
        "                    \"article\": None,\n",
        "                    \"relevance_score\": 0.95\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # Log the model using MLflow 3.0 syntax\n",
        "    mlflow.pyfunc.log_model(\n",
        "        name=\"enhanced_icc_rag_model\",\n",
        "        python_model=production_model,\n",
        "        input_example=input_example,\n",
        "        signature=infer_signature(input_example, output_example),\n",
        "        resources=[\n",
        "            DatabricksVectorSearchIndex(index_name=PAST_JUDGMENTS_INDEX),\n",
        "            DatabricksVectorSearchIndex(index_name=GENEVA_DOCUMENTATION_INDEX),\n",
        "            DatabricksServingEndpoint(endpoint_name=LLM_MODEL_ENDPOINT)\n",
        "        ],\n",
        "        pip_requirements=[\n",
        "            \"mlflow>=3.1.1\",\n",
        "            \"langchain\",\n",
        "            \"databricks-langchain\",\n",
        "            \"numpy\",\n",
        "            \"pandas\",\n",
        "            \"pydantic\"\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    # Register model in Unity Catalog\n",
        "    model_uri = f\"runs:/{run.info.run_id}/enhanced_icc_rag_model\"\n",
        "    registered_model = mlflow.register_model(\n",
        "        model_uri=model_uri,\n",
        "        name=\"enhanced_icc_rag_legal_research\"\n",
        "    )\n",
        "    \n",
        "    print(f\"✅ Model logged: {run.info.run_id}\")\n",
        "    print(f\"🔗 Model URI: {model_uri}\")\n",
        "    print(f\"📦 Model registered: {registered_model.name} v{registered_model.version}\")\n",
        "    print(f\"🌐 View in Unity Catalog: https://dbc-0619d7f5-0bda.cloud.databricks.com/explore/data/models/{registered_model.name}/version/{registered_model.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Examples & Deployment Instructions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_usage_examples():\n",
        "    \"\"\"Show comprehensive usage examples for the Enhanced ICC RAG System.\"\"\"\n",
        "    \n",
        "    print(\"🚀 ENHANCED ICC RAG SYSTEM - USAGE EXAMPLES\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(\"\\n📋 1. LOCAL USAGE:\")\n",
        "    print(\"\"\"\n",
        "# Initialize the system\n",
        "rag_system = EnhancedICCRAGSystem()\n",
        "\n",
        "# Simple legal research query\n",
        "question = \"What are the elements of crimes against humanity?\"\n",
        "context = rag_system.retrieve_context(question, top_k=8)\n",
        "analysis = rag_system.generate_legal_analysis(question, context)\n",
        "\n",
        "print(f\"Analysis: {analysis.analysis}\")\n",
        "print(f\"Confidence: {analysis.confidence_score}\")\n",
        "print(f\"Sources: {len(analysis.sources_used)}\")\n",
        "\"\"\")\n",
        "    \n",
        "    print(\"\\n📋 2. CONVERSATIONAL USAGE:\")\n",
        "    print(\"\"\"\n",
        "# Multi-turn conversation\n",
        "conversation_id = \"legal_research_session_001\"\n",
        "\n",
        "# First question\n",
        "question1 = \"How has the principle of proportionality been applied in ICTY judgments?\"\n",
        "context1 = rag_system.retrieve_context(question1, top_k=10)\n",
        "analysis1 = rag_system.generate_legal_analysis(question1, context1, conversation_id)\n",
        "\n",
        "# Follow-up question (with memory)\n",
        "question2 = \"What factors did the chamber consider in those cases?\"\n",
        "context2 = rag_system.retrieve_context(question2, top_k=8)\n",
        "analysis2 = rag_system.generate_legal_analysis(question2, context2, conversation_id)\n",
        "\"\"\")\n",
        "    \n",
        "    print(\"\\n📋 3. SERVING ENDPOINT USAGE:\")\n",
        "    print(\"\"\"\n",
        "# Deploy to serving endpoint\n",
        "import requests\n",
        "\n",
        "endpoint_url = \"https://your-workspace.cloud.databricks.com/serving-endpoints/enhanced-icc-rag/invocations\"\n",
        "headers = {\"Authorization\": \"Bearer YOUR_TOKEN\"}\n",
        "\n",
        "# Single query\n",
        "payload = {\n",
        "    \"dataframe_split\": {\n",
        "        \"columns\": [\"query\", \"num_results\", \"conversation_id\"],\n",
        "        \"data\": [[\"What are the requirements for combatant status?\", 10, \"session_001\"]]\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(endpoint_url, headers=headers, json=payload)\n",
        "result = response.json()[\"predictions\"][0]\n",
        "\n",
        "print(f\"Analysis: {result['analysis']}\")\n",
        "print(f\"Routing: {result['routing_decision']}\")\n",
        "print(f\"Sources: {result['sources_used']}\")\n",
        "print(f\"Confidence: {result['confidence_score']}\")\n",
        "\"\"\")\n",
        "    \n",
        "    print(\"\\n📋 4. BATCH PROCESSING:\")\n",
        "    print(\"\"\"\n",
        "# Multiple legal research questions\n",
        "batch_payload = {\n",
        "    \"dataframe_split\": {\n",
        "        \"columns\": [\"query\", \"num_results\", \"conversation_id\"],\n",
        "        \"data\": [\n",
        "            [\"Can you identify all ICTY judgments discussing civilian status?\", 12, \"batch_001\"],\n",
        "            [\"What factors determine active participation in hostilities?\", 10, \"batch_001\"],\n",
        "            [\"How do chambers assess subjective vs objective criteria?\", 8, \"batch_001\"]\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(endpoint_url, headers=headers, json=batch_payload)\n",
        "results = response.json()[\"predictions\"]\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"Question {i}: {result['routing_decision']} routing, {result['sources_used']} sources\")\n",
        "\"\"\")\n",
        "    \n",
        "    print(\"\\n📋 5. DEPLOYMENT INSTRUCTIONS:\")\n",
        "    print(\"\"\"\n",
        "# Step 1: Create serving endpoint\n",
        "# Go to Databricks UI > Serving > Create Endpoint\n",
        "# Select the registered model: enhanced_icc_rag_legal_research\n",
        "# Configure compute and scaling\n",
        "\n",
        "# Step 2: Test endpoint\n",
        "# Use the test queries provided above\n",
        "# Monitor performance and adjust scaling as needed\n",
        "\n",
        "# Step 3: Integration\n",
        "# Integrate with your legal research workflow\n",
        "# Use conversation_id for multi-turn research sessions\n",
        "# Monitor confidence scores for quality assurance\n",
        "\"\"\")\n",
        "    \n",
        "    print(\"\\n📋 6. OPTIMAL CONFIGURATION:\")\n",
        "    print(\"\"\"\n",
        "# Query types and recommended num_results:\n",
        "# - Complex legal research: 10-15\n",
        "# - Specific case law queries: 8-12  \n",
        "# - Geneva Convention queries: 6-10\n",
        "# - Factual questions: 4-8\n",
        "\n",
        "# Routing decisions:\n",
        "# - \"judgment\": ICTY/ICC case law queries\n",
        "# - \"geneva\": International humanitarian law queries  \n",
        "# - \"both\": Comparative legal analysis\n",
        "\n",
        "# Confidence scores:\n",
        "# - >0.8: High confidence, reliable analysis\n",
        "# - 0.6-0.8: Good confidence, review recommended\n",
        "# - <0.6: Low confidence, additional research needed\n",
        "\"\"\")\n",
        "\n",
        "# Show usage examples\n",
        "show_usage_examples()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
